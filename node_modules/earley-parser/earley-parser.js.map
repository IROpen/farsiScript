{"version":3,"sources":["earley-parser.litcoffee"],"names":["Grammar","ParserStore","Tokenizer","copyState","debugNestedArrays","debugState","equalArrays","getNext","bind","fn","me","apply","arguments","state","pos","rhs","length","lhs","ori","got","slice","array1","array2","entry1","entry2","index","l","len","RegExp","source","START","this","rules","defaults","addCategories","collapseBranches","showDebuggingOutput","expressionBuilder","tokenizer","comparator","JSON","equals","maxIterations","setOption","optionName","optionValue","addRule","base","categoryName","entry","len1","m","results1","sequence","sequences","call","Array","split","push","parse","input","options","copy","debug","expressionBuilderFlag","found","i","j","k","len2","len3","len4","len5","n","next","numIterationsDone","p","previous","q","r","recur","ref","ref1","ref2","ref3","ref4","ref5","ref6","ref7","result","results","rhss","s","skipped","stateGrid","stateSet","t","tmpi","tmpj","u","v","w","console","log","tokenize","test","hasOwnProperty","join","unshift","obj","args","o","ref8","indexOf","tokenTypes","addType","regexp","formatter","x","format","match","original","token","type","exec","Function","ary","stringify","map","y","key","xkeys","ykeys","Object","keys","sort","exports","WorkerGlobalScope","self","importScripts","addEventListener","event","P","category","func","funcre","index2","name","rest","startToken","text","data","replace","ctor","concat","postMessage"],"mappings":"AA8BI,IAAAA,QAAAC,YAAAC,UAAAC,UAAAC,kBAAAC,WAAAC,YAAAC,QAAAC,KAAA,SAAAC,EAAAC,GAAA,OAAA,WAAA,OAAAD,EAAAE,MAAAD,EAAAE,4BAAAL,QAAU,SAAEM,GACR,OAAGA,EAAMC,IAAMD,EAAME,IAAIC,OAAYH,EAAME,IAAIF,EAAMC,KAAU,MAKnEX,UAAY,SAAEU,UACVI,IAAMJ,EAAMI,IACZF,IAAMF,EAAME,IACZD,IAAMD,EAAMC,IACZI,IAAML,EAAMK,IACZC,IAAMN,EAAMM,IAAIC,MAAA,KAKpBd,YAAc,SAAEe,EAAQC,GACpB,IAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAA,GAAGN,EAAOL,SAAYM,EAAON,OAAY,OAAO,EAChD,IAAAS,EAAAC,EAAA,EAAAC,EAAAN,EAAAL,OAAAU,EAAAC,EAAAF,IAAAC,EAEI,UADAF,EAASF,EAAOG,GACbF,aAAkBK,QACjB,KAAGJ,aAAsBI,SACrBL,EAAOM,SAAYL,EAAOK,OAAY,OAAO,OAEjD,GAAGN,IAAYC,EAAY,OAAO,SAC1C,GA+BExB,QAAA,WAOY,SAAAA,EAAE8B,GAAAC,KAACD,MAADA,iHACZC,KAACC,SACDD,KAACE,UACGC,eAAgB,EAChBC,kBAAmB,EACnBC,qBAAsB,EACtBC,kBAAoB,KACpBC,UAAY,KACZC,WAAaC,KAAKC,OAClBC,eAAiB,sBAQzBC,UAAY,SAAEC,EAAYC,UACtBd,KAACE,SAASW,GAAcC,eAY5BC,QAAU,WACN,IAAAC,EAAAC,EAAAC,EAAAxB,EAAAC,EAAAC,EAAAuB,EAAAC,EAAAC,EAAAC,EAAAC,MADQN,EAAApC,UAAA,GACRwC,KAAA1B,EAAA,EAAAC,GADsB2B,EAAA,GAAA1C,UAAAI,OAAAI,MAAAmC,KAAA3C,UAAA,OACtBI,OAAAU,EAAAC,EAAAD,IAAA,CAKI,uBAJuBE,SACnByB,GAAaA,IACdA,aAAwBG,QACvBH,GAAW,GAAGA,GAAWI,MAAM,MACnChC,EAAA0B,EAAA,EAAAD,EAAAG,EAAArC,OAAAmC,EAAAD,EAAAzB,IAAA0B,qBACwBvB,SAChByB,EAAS5B,GAAS,IAAIG,OAAO,IAAIqB,EAAMpB,OAAO,cACtD,OAAAkB,EAAAhB,KAAAC,OAAAgB,GAAAD,EAASC,GAAAD,EAAAC,OAAuBU,KAAKL,0BA0C7CM,MAAQ,SAAEC,EAAOC,GACb,IAAAC,EAAAC,EAAAC,EAAAC,EAAA9C,EAAA+C,EAAAC,EAAAC,EAAA1C,EAAAC,EAAAuB,EAAAmB,EAAAC,EAAAC,EAAAC,EAAArB,EAAAsB,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAA3E,EAAA4E,EAAAC,EAAAC,EAAAhF,EAAAiF,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAoCA,cArCaxC,8BACbA,EAAQ3B,cAAiBH,KAACE,SAASC,0CACnC2B,EAAQ1B,iBAAoBJ,KAACE,SAASE,gDACtC0B,EAAQzB,oBAAuBL,KAACE,SAASG,iDACzCyB,EAAQxB,kBAAqBN,KAACE,SAASI,mBACvC2B,yBACAH,EAAQvB,UAAaP,KAACE,SAASK,gCAC/BuB,EAAQtB,WAAcR,KAACE,SAASM,oCAChCsB,EAAQnB,cAAiBX,KAACE,SAASS,gBACnCqB,EAAWF,EAAQzB,oBACf,kBAAGkE,QAAQC,IAAR5F,MAAA2F,QAAY1F,YAAkB,cAC/B,QAIH,MAAAiD,EAAAvB,WAAuC,iBAATsB,IAC7BA,EAAQC,EAAQvB,UAAUkE,SAAS5C,KAMvCkC,EAAA,yBAAc1C,KAAYc,EAAAxC,EAAA,EAAAuD,EAAArB,EAAA5C,OAAA,GAAAiE,EAAAvD,GAAAuD,EAAAvD,GAAAuD,EAAAf,EAAA,GAAAe,IAAAvD,IAAAA,sBAA1B,IAKU,GAAGgC,MACTzC,IAAM,GACNF,KAAQgB,KAACD,OACThB,IAAM,EACNI,IAAM,EACNC,SAIJwD,EAAoB,EACpBT,EAAAxC,EAAA,EAAAC,EAAAmE,EAAA9E,OAAAU,EAAAC,EAAAuC,IAAAxC,EAAA,CAII,WAHAqC,EAAM,uBAAuBG,EAAE,kCACbN,EAAM,MACxBG,EAAM,0BACMkC,EAAA9C,EAAA,EAAA8B,EAAAa,EAAA9E,OAAA,GAAAiE,EAAA9B,EAAA8B,EAAA9B,EAAA8B,EAAAgB,EAAA,GAAAhB,IAAA9B,IAAAA,EAAZ,CAGI,IAFAY,EAAM,kBAAkBkC,EAAK,KAC7BJ,EAAU,EACEK,EAAAzB,EAAA,EAAAS,EAAAY,EAAAG,GAAAjF,OAAA,GAAAkE,EAAAT,EAAAS,EAAAT,EAAAS,EAAAgB,EAAA,GAAAhB,IAAAT,IAAAA,EACLqB,EAAUG,GAAMjF,OAAS,IACzB8E,EAAUG,GAAMC,GAAMpF,IAAM,EAC3BiD,EAAM,kBAAkBmC,EAAK,KACvB7F,WAAWyF,EAAUG,GAAMC,KAEjCL,IACLA,EAAU,GACT9B,EAAM,cAAc8B,EAAQ,wBAQpC,IAPA9B,EAAM,0BAMNI,EAAI,EACEA,EAAI4B,EAAS/E,QAYf,GAXAH,EAAQkF,EAAS5B,GACjBJ,EAAM,SAASI,EAAE,IAAI9D,WAAWQ,IAQhC6D,EAAOnE,QAAQM,GACfkD,EAAM,QAASW,GACJ,OAARA,EAgCH,GAAGR,GAAKN,EAAM5C,OAAYmD,SAE1B,GADAJ,EAAM,oBAAqBW,aAAgB9C,QACxC8C,aAAgB9C,OAOZ8C,EAAK+B,KAAK7C,EAAMM,OACfJ,EAAO3D,UAAUU,IACZC,MACLgD,EAAK3C,IAAIuC,KAAKE,EAAMM,IACpB4B,EAAU5B,EAAE,GAAGR,KAAKI,GACpBC,EAAM,0BAAyBG,EAAE,GAAE,IAC/B7D,WAAWyD,KACnBK,QAdJ,CAgBA,IAAOpC,KAACC,MAAM0E,eAAehC,GACzB,KAAM,yCACAA,EAUV,IADAX,EAAM,WADN4B,EAAO5D,KAACC,MAAM0C,IACOiC,KAAK,OAAO,KACjCvC,EAAAU,EAAA,EAAAT,EAAAsB,EAAA3E,OAAA8D,EAAAT,EAAAD,IAAAU,EAAA,CAEI,WADAb,GAAQ,EACRc,EAAA,EAAAT,EAAAyB,EAAA/E,OAAA+D,EAAAT,EAAAS,IACI,YAAK9D,MAAOyD,GAASpE,YAAasF,EAAE7E,IAAKA,IACpB,IAAT6E,EAAE9E,IADd,CAEImD,GAAQ,EACR,MACDA,IACH8B,EAASrC,MACLzC,IAAMyD,EACN3D,IAAMA,EACND,IAAM,EACNI,IAAMgD,EACN/C,SACJ4C,EAAM,qBACF1D,WAAW0F,EAASA,EAAS/E,OAAO,MAEhD,GADAmD,IACGQ,KAAAU,EAAsBxB,EAAQnB,gBAA9B2C,EAA8C,EAC7C,KAAM,4CAhFV,CAWI,IAFAtB,EAAM,sDACUlD,EAAMK,KACtBkD,EAAAQ,EAAA,EAAA1B,GAAAiC,EAAAW,EAAAjF,EAAAK,MAAAF,OAAA4D,EAAA1B,EAAAkB,IAAAQ,EACI,UAAGrE,QAASqF,KAAO/E,EAAMI,MACrB2E,EAAIzF,UAAUyF,GACdA,EAAE9E,MACFK,EAAMN,EAAMM,IAAIC,MAAA,GACbyC,EAAQ3B,eACPf,EAAIyF,QAAQ/F,EAAMI,KACnB,MAAA4C,EAAAxB,mBACClB,EAAIyF,QAAQ5C,GACbH,EAAQ1B,kBACO,IAAdhB,EAAIH,SAAiBG,EAAMA,EAAI,IACnCyE,EAAEzE,IAAIuC,KAAKvC,GACX2E,EAAU5B,GAAGR,KAAKkC,GAClB7B,EAAM,2BAA2BG,EAAE,IAC/B7D,WAAWuF,IACZjB,KAAAS,EACAvB,EAAQnB,gBADR0C,EACwB,GACvB,KAAM,wCAElBjB,KAsDZ,IAHAJ,EAAM,kDACYH,EAAM,MACxBG,EAAM,0BACMkC,EAAAD,EAAA,EAAAV,EAAAQ,EAAA9E,OAAA,GAAAsE,EAAAU,EAAAV,EAAAU,EAAAV,EAAAW,EAAA,GAAAX,IAAAU,IAAAA,EAAZ,CAGI,IAFAjC,EAAM,kBAAkBkC,EAAK,KAC7BJ,EAAU,EACEK,EAAAC,EAAA,EAAAZ,EAAAO,EAAAG,GAAAjF,OAAA,GAAAuE,EAAAY,EAAAZ,EAAAY,EAAAZ,EAAAW,EAAA,GAAAX,IAAAY,IAAAA,EACLL,EAAUG,GAAMjF,OAAS,IACzB8E,EAAUG,GAAMC,GAAMpF,IAAM,EAC3BiD,EAAM,kBAAkBmC,EAAK,KACvB7F,WAAWyF,EAAUG,GAAMC,KAEjCL,IACLA,EAAU,GACT9B,EAAM,cAAc8B,EAAQ,wBAQpC,IAPA9B,EAAM,0BAMN2B,KACAU,EAAA,EAAA7B,GAAAiB,EAAAM,EAAAA,EAAA9E,OAAA,IAAAA,OAAAoF,EAAA7B,EAAA6B,IACI,GAAmB,cAAPnF,KAAqC,OAAvBV,QAASwF,GAAnC,CAMI,GALAN,EAASM,EAAS5E,IAAI,GAKnB,MAAA0C,EAAAxB,oBACC2C,EAAQ,SAAE6B,GACN,IAAAC,EAAAC,EAAA,OAAGF,aAAmBrD,OACnBqD,EAAI,KAAQ7C,GAGG,KADlB8C,EAAA,2BAAS1D,KAAAiD,EAAA,EAAA7B,GAAAwC,EAAAH,EAAAzF,MAAA,IAAAJ,OAAAqF,EAAA7B,EAAA6B,kBAAArB,EAAM+B,aAAf,IACQ/F,QAAgB6C,EAAQ1B,mBAC5B2E,EAAOA,EAAK,IAObA,EAAKG,aAAS,IAAe,OAAhC,EAEApD,EAAQxB,kBAAkByE,IAZfD,GAcR,OADPpB,EAAST,EAAMS,KACK,SAMxB,IADAxB,GAAQ,EACRoC,EAAA,EAAA7B,EAAAkB,EAAA1E,OAAAqF,EAAA7B,EAAA6B,IACI,UAAGxC,EAAQtB,WAAWsC,EAAUY,GAAhC,CACIxB,GAAQ,EACR,MACDA,GAAWyB,EAAQhC,KAAK+B,UAIvCC,KA9SF,GAwTAxF,UAAA,WACY,SAAAA,gFAAG6B,KAACmF,iCAsBlBC,QAAU,SAAEC,EAAQC,oBAAAA,EAAY,SAAEC,UAAOA,IACZ,MAAtBF,EAAOvF,OAAO,KACbuF,EAAS,IAAIxF,OAAO,OAAOwF,EAAOvF,OAAO,MAC7CE,KAACmF,WAAWxD,MACR0D,OAASA,EACTC,UAAYA,iBAqBpBb,SAAW,SAAE5C,GACT,IAAA2D,EAAA7F,EAAAC,EAAA6F,EAAA9C,EAAA+C,EAAAxC,EAAAQ,EAAAiC,EAAAC,EACA,IADAlC,KACM7B,EAAM5C,OAAS,GAArB,CAEI,IADAyG,EAAW7D,EAAM5C,OACjBU,EAAA,EAAAC,GAAAsD,EAAAlD,KAAAmF,YAAAlG,OAAAU,EAAAC,EAAAD,IACI,UAAO8F,EAAQG,EAAKP,OAAOQ,KAAKhE,GAAhC,CAEA,GADAA,EAAQA,EAAMxC,MAAAoG,EAAA,GAAAxG,QACX2G,EAAKN,qBAAqBQ,SAEtB,OADHnD,EAAOiD,EAAKN,UAAUG,EAAM,GAAIA,KAClB/B,EAAO/B,KAAKgB,OAF9B,CAMI,IAFA6C,EAAS,GAAGI,EAAKN,UACjBK,EAAQ,GACFhD,EAAO,aAAakD,KAAKL,IAC3BG,GAASH,EAAOnG,MAAA,EAAAsD,EAAAjD,OAAiB+F,EAAM9C,EAAK,IAC5C6C,EAASA,EAAOnG,MAAAsD,EAAAjD,MAAAiD,EAAA,GAAA1D,QACpByE,EAAO/B,KAAKgE,EAAQH,GACxB,MACJ,GAAG3D,EAAM5C,SAAUyG,EAAc,OAAO,YAC5ChC,KApEF,GA0ENrF,kBAAoB,SAAE0H,GAClB,OAAGA,aAAetE,OACX,OAAQhB,KAAKuF,UAAUD,EAAI,MAAQA,EAAMA,EAAI1G,MAAA,IAChD,IAAM0G,EAAIE,IAAK5H,mBAAoBuG,KAAM,KAAQ,KAEjDmB,GACRzH,WAAa,SAAEQ,SACX,IAAIA,EAAMI,IAAI,OAAMJ,EAAMC,IAAI,MAAKD,EAAME,IAAI,MAAKF,EAAMK,IAAI,SACtDd,kBAAkBS,EAAMM,MAwBlCqB,KAAKC,OAAS,SAAE6E,EAAGW,GAKf,IAAAC,EAAAxG,EAAAC,EAAAwG,EAAAC,EAAA,GAAed,aAAae,QAAgBJ,aAAaI,OAAzD,OAAO,EACP,GAAef,aAAa9D,OAAeyE,aAAazE,MAAxD,OAAO,EACP,KAAG8D,aAAiBe,QAAY,OAAOf,IAAKW,EAS5C,GAFAE,EAAUE,OAAOC,KAAKhB,GAAIiB,OAC1BH,EAAUC,OAAOC,KAAKL,GAAIM,OACX/F,KAAKuF,UAAUI,KAAe3F,KAAKuF,UAAUK,GAA5D,OAAO,EAKP,IAAA1G,EAAA,EAAAC,EAAAwG,EAAAnH,OAAAU,EAAAC,EAAAD,IACI,WAAOc,KAAKC,OAAO6E,EAAEY,GAAMD,EAAEC,IAAU,OAAO,SAClD,GAMD,oBAAAM,SAAA,OAAAA,UACCA,QAAQxI,QAAUA,QAClBwI,QAAQtI,UAAYA,YAOrB,oBAAAuI,mBAAA,OAAAA,mBAAsB,OAAA,oBAAAC,MAAA,OAAAA,KAAAA,KAAAC,mBAAA,MAKrB1I,eACAyI,KAAKE,iBAAiB,UAAW,SAAEC,GAC/B,IAAAC,EAAA/F,EAAAgG,EAAA9F,EAAA+F,EAAAC,EAAAxH,EAAAyH,EAAAxH,EAAAC,EAAAuB,EAAAC,EAAAqE,EAAA2B,EAAAlE,EAAAC,EAAAC,EAAAC,EAAAC,EAAA+B,EAAAgC,EAAA/F,EAAAC,EAAA+F,EAAAC,EAAA,OAAOT,EAAMU,KAAK,IAAlB,IAMS,mBACDtE,EAAgC4D,EAAMU,KAApCtE,EAAA,GAASkE,EAAAlE,EAAA,GAAMoE,EAAApE,EAAA,GACjBhF,YAAYkJ,GAAQ,IAAInJ,QAAQqJ,GARxC,IA0BS,UAED,GADAnE,EAAkC2D,EAAMU,KAAtCrE,EAAA,GAASiE,EAAAjE,EAAA,GAAMkC,EAAAlC,EAAA,GAAQ8D,EAAA9D,EAAA,GAClB,OAAA4D,EAAA7I,YAAAkJ,IAAgC,cACvCF,EAAS,oEAOND,KACCxB,EAAQyB,EAAOrB,KAAKoB,IACd,GAAKxB,EAAM,GAAGgC,QAAQ,MAAO,IACnCR,EAAO,SAAAA,EAAAlC,EAAA2C,+EAAA,CAAI5B,SAASL,EAAMpG,MAAA,GAAnB,kDACAkB,UAAa,IAAIpC,WAC5B4I,EAAE7G,SAASK,UAAU6E,QAAU,IAAIvF,OAAOwF,GAAU4B,GAzC5D,IAsDS,UAED,GADA7D,EAA4C0D,EAAMU,KAAhDpE,EAAA,GAASgE,EAAAhE,EAAA,GAAM4D,EAAA5D,EAAA,GAAU7B,EAAA,GAAA6B,EAAAnE,OAAAI,MAAAmC,KAAA4B,EAAA,MACpB,OAAA2D,EAAA7I,YAAAkJ,IAAgC,OACvC,IAAA1H,EAAAC,EAAA,EAAAC,EAAA2B,EAAAtC,OAAAU,EAAAC,EAAAF,IAAAC,EAMI,WALG,MAAM+E,KAAKpD,KACVC,EAAU7B,IAAW,IAAIG,OAAOyB,EAASjC,MAAA,MAC1CiC,aAAwBG,QACvBF,EAAU7B,GAAS4B,GACf,GAAGA,GAAWI,MAAM,MAC5ByF,EAAA/F,EAAA,EAAAD,EAAAG,EAAArC,OAAAmC,EAAAD,EAAAgG,IAAA/F,EACIiG,WAAahI,MAAA,GACE,MAAZ6B,EAAM,KACLmG,EAAO,IAAIxH,OAAO,IAAIwH,EAAK,MAC/B/F,EAAS6F,GAAUE,SAC3BN,EAAEhG,QAAFnC,MAAAmI,GAAUC,GAAUW,OAAAtI,MAAAmC,KAAAD,KApE5B,IA2ES,QAED,GADA8B,EAA0ByD,EAAMU,KAA9BnE,EAAA,GAAS+D,EAAA/D,EAAA,GAAMkE,EAAAlE,EAAA,GACV,OAAA0D,EAAA7I,YAAAkJ,IAAgC,cACvCT,KAAKiB,YAAYb,EAAEnF,MAAM2F,IA9EjC,IAmFS,sBACDjE,EAAoBwD,EAAMU,KAAxBlE,EAAA,GAAS8D,EAAA9D,EAAA,UACJpF,YAAYkJ","file":"earley-parser.js","sourcesContent":["\n# Parsing Module\n\n## Introduction\n\nThis module implements the Earley Parser, an algorithm [given on Wikipedia\nhere](https://en.wikipedia.org/wiki/Earley_parser).  Much of this code was\ntranslated from [the desktop version of Lurch](www.lurchmath.org).\n\n## Utilities\n\nAn Earley state is an object of the following form.  The `lhs` and `rhs`\ntogether are the rule currently being matched, `pos` is the current\nposition in that production, a zero-based index through all the interstitial\npoints in `rhs` (zero being before the whole thing, 1 after the first\nentry, etc.), `ori` the position in the input text at which the match\nbegan (called the origin), and `got` is the list of tokens parsed so far.\n```\n{\n    lhs : categoryname,\n    rhs : [ ... ],\n    pos : integerindex,\n    ori : integerindex,\n    got : [ ... ]\n}\n```\nA parsed token is either a plain string containing the terminal or an array\nwhose first element is the category name and the rest of which are the\nterminals and nonterminals in its parsing.\n\n    getNext = ( state ) ->\n        if state.pos < state.rhs.length then state.rhs[state.pos] else null\n\nThe following simple tool is used to copy state objects.  It is a shallow\ncopy in all except the `got` array.\n\n    copyState = ( state ) ->\n        lhs : state.lhs\n        rhs : state.rhs\n        pos : state.pos\n        ori : state.ori\n        got : state.got[..]\n\nWe will later need to compare two arrays of strings and/or regular\nexpressions for equality.  This function does so.\n\n    equalArrays = ( array1, array2 ) ->\n        if array1.length isnt array2.length then return no\n        for entry1, index in array1\n            entry2 = array2[index]\n            if entry1 instanceof RegExp\n                if entry2 not instanceof RegExp or \\\n                    entry1.source isnt entry2.source then return no\n            else\n                if entry1 isnt entry2 then return no\n        yes\n\n## Grammar class\n\nAll of the functionality of this module is embedded in a class called\n`Grammar`, which lets you define new grammars and then run them on strings\nto parse those strings.  This section defines that class.\n\nAs mentioned on the Wikipedia page linked to above, a grammar is a set of\nrules of the form `C -> A1 A2 ... An` where `C` is the name of a category\nand each `Ai` can be a category name or a terminal.\n\nThe `Grammar` class defined below stores a grammar as an object whose keys\nare category names with values of the following form.\n```\n    [\n        [ 'catname', 'catname', /terminal/, /terminal/, ... ],\n        [ 'catname', /terminal/, 'catname', /terminal/, ... ],\n        ...\n    ],\n```\nEach row in the two-dimensional array represents the right-hand side of one\nrule in the grammar, whose left hand side is the category name under which\nthe entire two-dimensional array is stored.\n\nThe entries in the arrays can be strings (which signify the names of\nnon-terminals) or regular expressions (which signify that they are\nterminals, which must match the regular expression).\n\nNow we begin the class.\n\n    class Grammar\n\n## Constructor\n\nIndicate which of the categories is the starting category by passing its\nname to a grammar when you construct one.\n\n        constructor : ( @START ) ->\n            @rules = { }\n            @defaults =\n                addCategories : yes\n                collapseBranches : no\n                showDebuggingOutput : no\n                expressionBuilder : null\n                tokenizer : null\n                comparator : JSON.equals\n                maxIterations : -1 # which means no maximum\n\nThe default options for the parsing algorithm are initialized in the\nconstructor above, but you can change them using the following routine.  The\nfirst parameter is the name of the option (from the list immediately above)\nand the second parameter is its new value.  The meaning of these options is\ndocumented [below](#earley-algorithm).\n\n        setOption : ( optionName, optionValue ) =>\n            @defaults[optionName] = optionValue\n\nAdd a rule to the grammar by specifying the category name and the sequence\nof Ai that appear on the right hand side.  This creates/extends the\ntwo-dimensional array described above.\n\nYou can pass more than one sequence by providing additional parameters, to\nadd them all at once.  You can also provide a string instead of an array,\nand it will be converted into an array by splitting at spaces as if it were\na string.  Regular expressions will be automatically wrapped in `^...$` for\nyou, so that they are always tested against the entire string.\n\n        addRule : ( categoryName, sequences... ) =>\n            for sequence in sequences\n                if sequence instanceof RegExp\n                    sequence = [ sequence ]\n                if sequence not instanceof Array\n                    sequence = \"#{sequence}\".split ' '\n                for entry, index in sequence\n                    if entry instanceof RegExp\n                        sequence[index] = new RegExp \"^#{entry.source}$\"\n                ( @rules[categoryName] ?= [ ] ).push sequence\n\n## Earley Algorithm\n\nThe following function is the workhorse of this module.  It assumes that the\ninput is a string of a nonzero length.  Options is not a required parameter,\nbut if it is present it should be an object with some subset of the\nfollowing properties.  Any unspecified properties take the defaults given in\nthe constructor for this class, unless you changed them with `setOption`,\ndefined [above](#constructor).\n * `addCategories : true` iff category names should be prepended to each\n   match sequence\n * `collapseBranches : true` iff one-argument match sequences should be\n   collapsed, as in `[[[[a]]]] -> a`\n * `showDebuggingOutput : true` iff lots of debugging spam should be dumped\n   to the console as the algorithm executes\n * `expressionBuilder` can be set to a function that will be called each\n   time a production is completed.  It will receive as input the results of\n   that production (wrapped in an array if `collapseBranches` is true, with\n   the category name prepended if `addCategories` is true) and it can return\n   any object to replace that array in the final result.  Since this will be\n   called at every level of the hierarchy, you can use this to recursively\n   build expressions from the leaves upwards.  Because it will need to be\n   copyable, outputs are restricted to JSON data.\n * `tokenizer` can be an instance of the `Tokenizer` class\n   [defined later in this module](#tokenizing), and if it is, it will be\n   applied to any string input received by the parser before the parser does\n   anything with it.  This way you can simply place the tokenizer inside the\n   parser and forget about it; it will be run automatically.\n * `comparator` is used to compare two results before returning the full\n   list, so that duplicates can be removed.  This defaults to a JSON-based\n   comparison, but will therefore go into an infinite loop for circular\n   structures.  Feel free to provide a different one if the default does not\n   meet your needs.  To return duplicates, simply set this to `-> no`.\n * `maxIterations` defaults to infinite, but can be specified as a positive\n   integer, and the parsing algorithm will not iterate its innermost loops\n   any more than this many times.  This can be useful if you have a\n   suspected infinite loop in a grammar, and want to debug it.\n\nThis algorithm is documented to some degree, but it will make much more\nsense if you have read the Wikipedia page cited at the top of this file.\n\n        parse : ( input, options = { } ) =>\n            options.addCategories ?= @defaults.addCategories\n            options.collapseBranches ?= @defaults.collapseBranches\n            options.showDebuggingOutput ?= @defaults.showDebuggingOutput\n            options.expressionBuilder ?= @defaults.expressionBuilder\n            expressionBuilderFlag = { }\n            options.tokenizer ?= @defaults.tokenizer\n            options.comparator ?= @defaults.comparator\n            options.maxIterations ?= @defaults.maxIterations\n            debug = if options.showDebuggingOutput then \\\n                -> console.log arguments... else ->\n            debug '\\n\\n'\n\nRun the tokenizer if there is one, and the input needs it.\n\n            if options.tokenizer? and typeof input is 'string'\n                input = options.tokenizer.tokenize input\n\nInitialize the set of states to the array `[ [], [], ..., [] ]`, one entry\nfor each interstice between characters in `input`, including one for before\nthe first character and one for after the last.\n\n            stateGrid = ( [] for i in [0..input.length] )\n\nPush all productions for the starting non-terminal onto the initial state\nset.\n\n            stateGrid[0].push\n                lhs : ''\n                rhs : [ @START ]\n                pos : 0\n                ori : 0\n                got : []\n\nDo the main nested loop which solves the whole problem.\n\n            numIterationsDone = 0\n            for stateSet, i in stateGrid\n                debug \"processing stateSet #{i} in this stateGrid\n                    (with input #{input}):\"\n                debug '----------------------'\n                for tmpi in [0...stateGrid.length]\n                    debug \"|    state set #{tmpi}:\"\n                    skipped = 0\n                    for tmpj in [0...stateGrid[tmpi].length]\n                        if stateGrid[tmpi].length < 15 or \\\n                           stateGrid[tmpi][tmpj].pos > 0\n                            debug \"|        entry #{tmpj}:\n                                #{debugState stateGrid[tmpi][tmpj]}\"\n                        else\n                            skipped++\n                    if skipped > 0\n                        debug \"|    (plus #{skipped} at pos 0 not shown)\"\n                debug '----------------------'\n\nThe following loop is written in this indirect way (not using `for`) because\nthe contents of `stateSet` may be modified within the loop, so we need to be\nsure that we do not pre-compute its length, but allow it to grow.\n\n                j = 0\n                while j < stateSet.length\n                    state = stateSet[j]\n                    debug \"entry #{j}:\", debugState state\n\nThere are three possibilities.\n * The next state is a terminal,\n * the next state is a production, or\n * there is no next state.\nEach of these is handled by a separate sub-task of the Earley algorithm.\n\n                    next = getNext state\n                    debug 'next:', next\n                    if next is null\n\nThis is the case in which there is no next state.  It is handled by running\nthe \"completer\":  We just completed a nonterminal, so mark progress in\nwhichever rules spawned it by copying them into the next column in\n`stateGrid`, with progress incremented one step.\n\nThen we proceed with the code for the completer.\n\n                        debug 'considering if this completion matters to\n                            state set', state.ori\n                        for s, k in stateGrid[state.ori]\n                            if getNext( s ) is state.lhs\n                                s = copyState s\n                                s.pos++\n                                got = state.got[..]\n                                if options.addCategories\n                                    got.unshift state.lhs\n                                if options.expressionBuilder?\n                                    got.unshift expressionBuilderFlag\n                                if options.collapseBranches and \\\n                                    got.length is 1 then got = got[0]\n                                s.got.push got\n                                stateGrid[i].push s\n                                debug \"completer added this to #{i}:\",\n                                    debugState s\n                                if numIterationsDone++ > \\\n                                   options.maxIterations > 0\n                                    throw 'Maximum number of iterations\n                                        reached.'\n                        j++\n                        continue\n                    if i >= input.length then j++ ; continue\n                    debug 'is it a terminal?', next instanceof RegExp\n                    if next instanceof RegExp\n\nThis is the case in which the next state is a terminal.  It is handled by\nrunning the \"scanner\":  If the next terminal in `state` is the one we see\ncoming next in the input string, then find every production at that\nterminal's origin that contained that terminal, and mark progress here.\n\n                        if next.test input[i]\n                            copy = copyState state\n                            copy.pos++\n                            copy.got.push input[i]\n                            stateGrid[i+1].push copy\n                            debug \"scanner added this to #{i+1}:\",\n                                debugState copy\n                        j++\n                        continue\n                    if not @rules.hasOwnProperty next\n                        throw \"Unknown non-terminal in grammar rule:\n                            #{next}\"\n\nThis is the case in which the next state is a non-terminal, i.e., the lhs of\none or more rules.  It is handled by running the \"predictor:\"  For every\nrule that starts with the non-terminal that's coming next, add that rule to\nthe current state set so that it will be explored in future passes through\nthe inner of the two main loops.\n\n                    rhss = @rules[next]\n                    debug \"rhss: [#{rhss.join('],[')}]\"\n                    for rhs, k in rhss\n                        found = no\n                        for s in stateSet\n                            if s.lhs is next and equalArrays( s.rhs, rhs ) \\\n                                    and s.pos is 0\n                                found = yes\n                                break\n                        if not found\n                            stateSet.push\n                                lhs : next\n                                rhs : rhs\n                                pos : 0\n                                ori : i\n                                got : []\n                            debug 'adding this state:',\n                                debugState stateSet[stateSet.length-1]\n                    j++\n                    if numIterationsDone++ > options.maxIterations > 0\n                        throw 'Maximum number of iterations reached.'\n            debug \"finished processing this stateGrid\n                (with input #{input}):\"\n            debug '----------------------'\n            for tmpi in [0...stateGrid.length]\n                debug \"|    state set #{tmpi}:\"\n                skipped = 0\n                for tmpj in [0...stateGrid[tmpi].length]\n                    if stateGrid[tmpi].length < 15 or \\\n                       stateGrid[tmpi][tmpj].pos > 0\n                        debug \"|        entry #{tmpj}:\n                            #{debugState stateGrid[tmpi][tmpj]}\"\n                    else\n                        skipped++\n                if skipped > 0\n                    debug \"|    (plus #{skipped} at pos 0 not shown)\"\n            debug '----------------------'\n\nThe main loop is complete.  Any completed production in the final state set\nthat's marked as a result (and thus coming from state 0 to boot) is a valid\nparsing and should be returned.  We find such productions with this loop:\n\n            results = [ ]\n            for stateSet in stateGrid[stateGrid.length-1]\n                if stateSet.lhs is '' and getNext( stateSet ) is null\n                    result = stateSet.got[0]\n\nWhen we find one, we have some checks to do before returning it.  First,\nrecursively apply `expressionBuilder`, if the client asked us to.\n\n                    if options.expressionBuilder?\n                        recur = ( obj ) ->\n                            if obj not instanceof Array or \\\n                               obj[0] isnt expressionBuilderFlag\n                                return obj\n                            args = ( recur o for o in obj[1..] )\n                            if args.length is 1 and options.collapseBranches\n                                args = args[0]\n\nIf the expression builder function returns undefined for any subexpression\nof the whole, we treat that as an error (saying the expression cannot be\nbuilt for whatever application-specific reason the builder function has) and\nwe thus do not include that result in the list.\n\n                            if args.indexOf( undefined ) > -1\n                                return undefined\n                            options.expressionBuilder args\n                        result = recur result\n                        if not result? then continue\n\nSecond, don't return any duplicates.  So check to see if we've already seen\nthis result before we add it to the final list of results to return.\n\n                    found = no\n                    for previous in results\n                        if options.comparator previous, result\n                            found = yes\n                            break\n                    if not found then results.push result\n\nNow return the final result list.\n\n            results\n\n## Tokenizing\n\nWe also provide a class for doing simple tokenization of strings into arrays\nof tokens, which can then be passed to a parser.  To use this class, create\nan instance, add some token types using the `addType` function documented\nbelow, then either call its `tokenize` function yourself on a string, or\njust set this tokenizer as the default tokenizer on a parser.\n\n    class Tokenizer\n        constructor : -> @tokenTypes = [ ]\n\nThis function adds a token type to this object.  The first parameter is the\nregular expression used to match the tokens.  The second parameter can be\neither of three things:\n * If it is a function, that function will be run on every instance of the\n   token that's found in any input being tokenized, and the output of the\n   function used in place of the token string in the return value from this\n   tokenizer.  But if the function returns null, the tokenizer will omit\n   that token from the output array.  This is useful for, say, removing\n   whitespace:  `addType( /\\s/, -> null )`.  The function will actually\n   receive two parameters, the second being the regular expresison match\n   object, which can be useful if there were captured subexpressions.\n * If it is a string, that string will be used as the output token instead\n   of the actual matched token.  All `%n` patterns in the output will be\n   simultaneously replaced with the captured expressions of the type's\n   regular expression (with zero being the entire match).  This is useful\n   for reformatting tokens by adding detail.  Example:\n   `addType( /-?[0-9]+/, 'Integer(%0)' )`\n * The second parameter may be omitted, and it will be treated as the\n   identity function, as in the first bullet point above.\n\n        addType : ( regexp, formatter = ( x ) -> x ) =>\n            if regexp.source[0] isnt '^'\n                regexp = new RegExp \"^(?:#{regexp.source})\"\n            @tokenTypes.push\n                regexp : regexp\n                formatter : formatter\n\nTokenizing is useful for grouping large, complex chunks of text into one\npiece before parsing, so that the parsing rules can be simpler and clearer.\nFor example, a regular expression that groups double-quoted string literals\ninto single tokens is `/\"(?:[^\\\\\"]|\\\\\\\\|\\\\\")*\"/`.  That's a much shorter bit\nof code to write than a complex set of parsing rules that accomplish the\nsame purpose; it will also run more efficiently than those rules would.\n\nThe following routine tokenizes the input, returning one of two things:\n * an array of tokens, each of which was the output of the formatter\n   function/string provided to `addType()`, above, or\n * null, because some portion of the input string did not match any of the\n   token types added with `addType()`.\n\nThe routine simply tries every regular expression of every token type added\nwith `addType()`, above, and when one succeeds, it pops that text off the\ninput string, saving it to a results list after passing it through the\ncorresponding formatter.  If at any point none of the regular expressions\nmatches the beginning of the remaining input, null is returned.\n\n        tokenize : ( input ) =>\n            result = [ ]\n            while input.length > 0\n                original = input.length\n                for type in @tokenTypes\n                    if not match = type.regexp.exec input then continue\n                    input = input[match[0].length..]\n                    if type.formatter instanceof Function\n                        next = type.formatter match[0], match\n                        if next? then result.push next\n                    else\n                        format = \"#{type.formatter}\"\n                        token = ''\n                        while next = /\\%([0-9]+)/.exec format\n                            token += format[...next.index] + match[next[1]]\n                            format = format[next.index+next[0].length..]\n                        result.push token + format\n                    break\n                if input.length is original then return null\n            result\n\n## Debugging\n\nThe following debugging routines are used in some of the code above.\n\n    debugNestedArrays = ( ary ) ->\n        if ary instanceof Array\n            if '{}' is JSON.stringify ary[0] then ary = ary[1...]\n            '[' + ary.map( debugNestedArrays ).join( ',' ) + ']'\n        else\n            ary\n    debugState = ( state ) ->\n        \"(#{state.lhs} -> #{state.pos}in[#{state.rhs}], #{state.ori}) got\n            #{debugNestedArrays state.got}\"\n\n## Comparing JSON objects for equality\n\nBy a \"JSON object\" I mean an object where the only information we care about\nis that which would be preserved by `JSON.stringify` (i.e., an object that\ncan be serialized and deserialized with JSON's `stringify` and `parse`\nwithout bringing any harm to our data).\n\nWe wish to be able to compare such objects for semantic equality (not actual\nequality of objects in memory, as `==` would do).  We cannot simply do this\nby comparing the `JSON.stringify` of each, because [documentation on\nJSON.stringify](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/stringify)\nsays that we cannot rely on a consistent ordering of the object keys.  Thus\nwe implement the following comparison routine.\n\nNote that this only works for objects that fit the requirements above; if\nequality (in your situation) is affected by the prototype chain, or if your\nobject contains functions, or any other similar difficulty, then this\nroutine is not guaranteed to work for you.\n\nIt yields the same result as `JSON.stringify(x) is JSON.stringify(y)` would\nif `stringify` always gave the same ordering of object keys.\n\n    JSON.equals = ( x, y ) ->\n\nIf only one is an object, or only one is an array, then they're not equal.\nIf neither is an object, you can use plain simple `is` to compare.\n\n        return no if ( x instanceof Object ) isnt ( y instanceof Object )\n        return no if ( x instanceof Array ) isnt ( y instanceof Array )\n        if x not instanceof Object then return x is y\n\nSo now we know that both inputs are objects.\n\nGet their keys in a consistent order.  If they aren't the same for both\nobjects, then the objects aren't equal.\n\n        xkeys = ( Object.keys x ).sort()\n        ykeys = ( Object.keys y ).sort()\n        return no if ( JSON.stringify xkeys ) isnt ( JSON.stringify ykeys )\n\nIf there's any key on which the objects don't match, then they aren't equal.\nOtherwise, they are.\n\n        for key in xkeys\n            if not JSON.equals x[key], y[key] then return no\n        yes\n\n## Context\n\nThe following step ensures that this file works in Node.js, for testing.\n\n    if exports?\n        exports.Grammar = Grammar\n        exports.Tokenizer = Tokenizer\n\nAnd the following lines test to see if this function is running in a\n[WebWorker](https://www.w3.org/TR/workers/), and if so, they install an\nevent handler for messages posted from the main thread, which exposes the\nkey API from this module to the outside, through message-passing.\n\n    if WorkerGlobalScope? or self?.importScripts?\n\nWe keep track of a set of named parsers in this object.  Clients can create\nthem by passing messages to this thread, as defined below.\n\n        ParserStore = { }\n        self.addEventListener 'message', ( event ) ->\n            switch event.data[0]\n\nReceiving a message of the form\n`[ 'newParser', 'parser name', 'start token' ]`\ncreates a new parser.\n\n                when 'newParser'\n                    [ command, name, startToken ] = event.data\n                    ParserStore[name] = new Grammar startToken\n\nClients can add types to the parser's tokenizer (which is created if there\nwasn't one before) with messages of the form\n`[ 'addType', 'parser name', 'regular expression' ]`.  With two\nexceptions, these messages are converted directly into function calls of\n`addType()` in the tokenizer (so see its documentation above).\n\nThe exceptions:  First, because regular expressions cannot be passed to\nworkers, the client must pass `regexp.source` instead, and on this end, the\n`RegExp` constructor will be called to rebuild the object.\n\nSecond, there is an optional fourth argument, the transformation function to\nbe applied to any token encountered of this type.  Because functions cannot\nbe passed to workers, the client must convert the function to a string\n(e.g., `String(f)` or CoffeeScript `\"#{f}\"`).  It will be rebuilt into a\nfunction on this side, obviouslw without its original environment/scope.\n\n                when 'addType'\n                    [ command, name, regexp, func ] = event.data\n                    if not ( P = ParserStore[name] )? then return\n                    funcre = ///\n                        \\s*function\\s*\n                        \\(((?:[a-zA-Z0-9,]|\\s)*)\\)\n                        \\s*\\{\\s*\n                        ((?:.|\\n)*)\n                        \\}\\s*$\n                        ///\n                    if func\n                        match = funcre.exec func\n                        match[1] = match[1].replace /\\s/g, ''\n                        func = new Function match[1..]...\n                    P.defaults.tokenizer ?= new Tokenizer()\n                    P.defaults.tokenizer.addType ( new RegExp regexp ), func\n\nClients can add rules to the parser with messages of the form\n`[ 'addRule', 'parser name', 'category', sequences... ]`.  These\nmessages are converted directly into function calls of `addRule()` in the\nparser, so see its documentation above.\n\nBecause regular expressions cannot be passed to WebWorkers, we modify the\nconvention in storing the sequences.  Each item in a sequence must be of\nthe form \"c:category name\" or \"t:terminal regexp\" so that category names\nand regular expressions for terminals can be distinguished.  We convert\nthem to strings or regular expressions before calling `addRule()`.\n\n                when 'addRule'\n                    [ command, name, category, sequences... ] = event.data\n                    if not ( P = ParserStore[name] )? then return\n                    for sequence, index in sequences\n                        if /^t:/.test sequence\n                            sequences[index] = [ new RegExp sequence[2..] ]\n                        if sequence not instanceof Array\n                            sequences[index] = sequence =\n                                \"#{sequence}\".split ' '\n                        for entry, index2 in sequence\n                            rest = entry[2..]\n                            if entry[0] is 't'\n                                rest = new RegExp \"^#{rest}$\"\n                            sequence[index2] = rest\n                    P.addRule category, sequences...\n\nClients passing messages of the form `[ 'parse', 'parser name', 'text' ]`\nare requesting the named parser to parse the given text and then send a\nmessage back containing the results (which may be an empty list, as in the\ndocumentation of the `parse()` function in the `Parser` class, above).\n\n                when 'parse'\n                    [ command, name, text ] = event.data\n                    if not ( P = ParserStore[name] )? then return\n                    self.postMessage P.parse text\n\nClients can pass a message `[ 'deleteParser', 'parser name' ]` to remove\nthe named parser from memory.\n\n                when 'deleteParser'\n                    [ command, name ] = event.data\n                    delete ParserStore[name]\n"]}