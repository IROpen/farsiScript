{
    "docs": [
        {
            "location": "/", 
            "text": "Earley Parser Documentation\n\n\nGet started now\n\n\nTo import this code via a CDN or \nnpm\n, check out the \nAPI Reference\nPage\n.\n\n\nBackground\n\n\nThe \nEarley Parsing algorithm\n\ncan parse strings according to any context-free grammar.  This module lets\nthe user define tokens and grammar rules, then pass strings in to be\ntokenized and parsed, or just pass in pre-tokenized arrays to be parsed.\n\n\nParse trees are returned as nested JavaScript arrays, but the user can also\nprovide callbacks that construct other kinds of hierarchical structures (or\ndo computation) instead of building parse trees.\n\n\nMore Information\n\n\nThe following additional information is available in this documentation.\n\n\n\n\nSource Code\n - see the source and/or import it into your\n   own project\n\n\nAPI Reference\n - how to use the various functions and\n   objects provided\n\n\n\n\nMore documentation may be added here in the future. For now you can \nview this repository \n.", 
            "title": "Home"
        }, 
        {
            "location": "/#earley-parser-documentation", 
            "text": "", 
            "title": "Earley Parser Documentation"
        }, 
        {
            "location": "/#get-started-now", 
            "text": "To import this code via a CDN or  npm , check out the  API Reference\nPage .", 
            "title": "Get started now"
        }, 
        {
            "location": "/#background", 
            "text": "The  Earley Parsing algorithm \ncan parse strings according to any context-free grammar.  This module lets\nthe user define tokens and grammar rules, then pass strings in to be\ntokenized and parsed, or just pass in pre-tokenized arrays to be parsed.  Parse trees are returned as nested JavaScript arrays, but the user can also\nprovide callbacks that construct other kinds of hierarchical structures (or\ndo computation) instead of building parse trees.", 
            "title": "Background"
        }, 
        {
            "location": "/#more-information", 
            "text": "The following additional information is available in this documentation.   Source Code  - see the source and/or import it into your\n   own project  API Reference  - how to use the various functions and\n   objects provided   More documentation may be added here in the future. For now you can  view this repository  .", 
            "title": "More Information"
        }, 
        {
            "location": "/source-code/", 
            "text": "Source Code\n\n\nReading the source\n\n\nThe code in \nthe repository\n\nresides in \none\nfile\n,\nwritten in \nLiterate CoffeeScript\n.\n\n\nChanging the source\n\n\nIf you don't like that language, you can always compile it directly to\nJavaScript with the following command.\n\n\ncoffee --compile earley-parser.litcoffee\n\n\n\n\nThis assumes that you've \ninstalled\nCoffeeScript\n and have the \nsource\nfile\n\naccessible.\n\n\nImporting the source\n\n\nTo import the source into your project, you can include it directly from a\nCDN at \nthis\nURL\n. There is a\nsource map file in the same folder that your browser should detect.", 
            "title": "Source"
        }, 
        {
            "location": "/source-code/#source-code", 
            "text": "", 
            "title": "Source Code"
        }, 
        {
            "location": "/source-code/#reading-the-source", 
            "text": "The code in  the repository \nresides in  one\nfile ,\nwritten in  Literate CoffeeScript .", 
            "title": "Reading the source"
        }, 
        {
            "location": "/source-code/#changing-the-source", 
            "text": "If you don't like that language, you can always compile it directly to\nJavaScript with the following command.  coffee --compile earley-parser.litcoffee  This assumes that you've  installed\nCoffeeScript  and have the  source\nfile \naccessible.", 
            "title": "Changing the source"
        }, 
        {
            "location": "/source-code/#importing-the-source", 
            "text": "To import the source into your project, you can include it directly from a\nCDN at  this\nURL . There is a\nsource map file in the same folder that your browser should detect.", 
            "title": "Importing the source"
        }, 
        {
            "location": "/api-reference/", 
            "text": "API Reference\n\n\nGetting started\n\n\nIn the browser\n\n\nImport the minified JavaScript, which you can \ndownload from our repository\ndirectly\n\nor import from a CDN with the following one-liner.\n\n\nscript src='https://cdn.jsdelivr.net/npm/earley-parser@1/earley-parser.js'\n/script\n\n\n\n\n\nFrom the command line\n\n\nOr install this package into your project the usual way:\n\n\nnpm install earley-parser\n\n\n\n\nThen within any of your modules, import it as follows.\n\n\nTokenizer = require( \nearley-parser\n ).Tokenizer;\nGrammar = require( \nearley-parser\n ).Grammar;\n\n\n\n\nAfter that, any of the example code snippets in this documentation should\nfunction as-is.\n\n\nIn a \nWebWorker\n\n\nTo place this script in a WebWorker, you will need to download \nits source\nfile\n and place it in your project's web space.\n\n\nYour script can then create the worker as follows.\n\n\nW = new Worker( \npath/to/earley-parser.js\n );\n\n\n\n\nThis exposes an asynchronous API documented \nbelow\n.\n\n\nTokenizing\n\n\nTraditionally, parsing text is split first into a \"tokenization\" phase, in\nwhich chunks of text are recognized as atomic units, and thus the string\nbecomes an array of its substrings, each of size 1 or greater, followed by\nthe parsing phase, which operates on that flat array, arranging it into a\ntree structure.\n\n\nFor example, in standard arithmetic, the text \"1+23*5\" might first be\ntokenized into the array \n[ \"1\", \"+\", \"23\", \"*\", \"5\" ]\n, and then parsed\ninto the (prefix-notation) tree structure\n\n[ \"+\", \"1\", [ \"*\", \"23\", \"5\" ] ]\n.\n\n\nA tokenizer is therefore an ordered list of regular expressions for\ndetecting tokens, popping them off of an input string, and then possibly\nmanipulating them before adding them to a growing array of tokens found in\nthe string.  This module provides a class for creating tokenizers.\n\n\nConstructor\n\n\nThe constructor takes no parameters, so it is very simple to use.\n\n\nT = new Tokenizer();\n\n\n\n\nAdding types to a tokenizer\n\n\nThere is precisely one function for adding types to the tokenizer.  Note,\nhowever, that types are checked in the order in which they were added to the\ntokenizer, so when you call this function repeatedly to add various types of\ntokens, you should take care to order the calls correctly.\n\n\nFor instance, perhaps you are writing a tokenizer for simple algebraic\nexpressions, in which a sequence of letters will be seen as the\nmultiplication of variables (i.e., \"abc\" means \"a times b times c\") except\nfor a few special sequences of letters such as \"sin\" and \"cos\" and \"pi\" and\nperhaps a few others.\n\n\nIt is important to add the tokens for \"sin\" and \"cos\" and so on first, and\nthen the generic single-letter token thereafter, so that the special cases\nhave a chance to be applied before the general case.  Otherwise the general\ncase would catch all sequences of letters, and the special cases would\nnever have a chance to be applied.\n\n\nThe function signature looks like so:\n\n\nT.addType( regexp, formatter );\n\n\n\n\nThe two parameters are documented thoroughly in\n\nthe source code documentation\n, so I do not repeat the information here.\n\n\nTokenizing\n\n\nTo tokenize text, simply call \nT.tokenize( input )\n on the text.\n\n\nExample:\n\n\n\nT = new Tokenizer();\nT.addType( /sin/ );\nT.addType( /cos/ );\nT.addType( /pi/ );\nT.addType( /[a-z]/, function ( name ) { return \"Variable:\" + name; } );\nT.addType( /\\s/, function () { return null; } );\nconsole.log( T.tokenize( 'sin x' ) );\nconsole.log( T.tokenize( 'cospiy' ) );\n\n\n\n\nParsing\n\n\nA grammar is a set of rules defining the language to parse.  For more\ninformation on context-free grammars, see \nthe Wikipedia article on the\nEarley parser\n.\n\n\nFor the sake of having a concrete running example in this section, let's\nassume we want to want to create the extremely simple grammar used as \nan\nexample in that same\narticle\n.  We will make\none modification: we will accept any integer, rather than just the four\ndigits in that example.  Our grammar can thus be summarized as the following\nrules.\n\n\n\n\nP ::= S\n\n\nS ::= S+M | M\n\n\nM ::= M*T | T\n\n\nT ::= any integer\n\n\n\n\nWe can represent the same grammar without the | symbol by separating single\nlines into two separate lines.\n\n\n\n\nP ::= S\n\n\nS ::= S+M\n\n\nS ::= M\n\n\nM ::= M*T\n\n\nM ::= T\n\n\nT ::= any integer\n\n\n\n\nEither way of representing a grammar is acceptable, and supported by this\nmodule.  See \nthe section on specifying grammar\nrules\n, below.\n\n\nConstructor\n\n\nThere is just one constructor for grammars, and it takes as its sole\nargument the name of the start nonterminal.  This need not be a single\ncapital letter, as it is in the example above; nonterminals can have any\nword as their name.\n\n\nG = new Grammar( 'P' );\n\n\n\n\nSetting default options\n\n\nAfter constructing a new grammar, you can choose to set some default options\nthat will govern its behavior.  Any of these can be overridden in any call\nto the parse function, later, but you can set defaults here if you plan to\nneed them often.  You call \nG.setOption( name, value )\n to set the default\nvalue for any option.\n\n\nThe options are documented thoroughly in \nthe source code\ndocumentation\n,\nso I do not repeat that information here.  Examples of the output produced\nby the various options appears in \nthe parsing\nsection\n, below.\n\n\nAdding grammar rules\n\n\nA grammar rule requires a left-hand side, which must be a single\nnonterminal, represented by a string.  Its right-hand side is typically an\narray.  For instance, the grammar rule M ::= M\nT has M as its left-hand side\nand the three-element array M, \n, and T as its right hand side.\n\n\nThe elements on the right hand side come in two types.  There are other\nnonterminals (such as M and T) and there are terminals (the symbol \n, in\nthis example).  Nonterminals are represented as strings, and terminals as\nregular expressions.  Thus to create the grammar rule M ::= M\nT, we would\nuse \n'M'\n as the left-hand side and \n[ 'M', /\\*/, 'T' ]\n as the right-hand\nside.\n\n\nOur complete example grammar can then be created as follows.\n\n\nG = new Grammar( 'P' );\nG.addRule( 'P', [ 'S' ] );\nG.addRule( 'S', [ 'S', /\\+/, 'M' ] );\nG.addRule( 'S', [ 'M' ] );\nG.addRule( 'M', [ 'M', /\\*/, 'T' ] );\nG.addRule( 'M', [ 'T' ] );\nG.addRule( 'T', [ /-?[0-9]+/ ] );\n\n\n\n\nThere are a few things to improve upon here.\n\n\n\n\nWe may combine rules that have the same left-hand side, just be listing\n    the right-hand sides one after the other.\n\n\nWe may express a one-element array containing a terminal by the regular\n    expression alone, omitting the enclosing array.\n\n\nWe may express a right-hand side consisting exclusively of one or more\n    nonterminals by writing it as a single string, with the names of the\n    nonterminals separated by spaces.\n\n\n\n\nThis reduces our grammar somewhat.\n\n\nG = new Grammar( 'P' );\nG.addRule( 'P', 'S' );\nG.addRule( 'S', [ 'S', /\\+/, 'M' ], 'M' );\nG.addRule( 'M', [ 'M', /\\*/, 'T' ], 'T' );\nG.addRule( 'T', /-?[0-9]+/ );\n\n\n\n\nThe P in this grammar is redundant; we could have left it out and declared\nour grammar to start with S, but I leave it in to be consistent with the\nWikipedia article from which it was taken.\n\n\nRunning the parser\n\n\nWe can then parse text by passing it to the grammar's \nparse\n member\nfunction.\n\n\n\nT = new Tokenizer();\nT.addType( /-?[0-9]+/ );\nT.addType( /[+*]/ );\nT.addType( /\\s/, function () { return null; } );\nG = new Grammar( 'P' );\nG.setOption( 'tokenizer', T );\n// Comment out either of these lines and re-run to see their effects:\nG.setOption( 'addCategories', false );\nG.setOption( 'collapseBranches', true );\nG.addRule( 'P', 'S' );\nG.addRule( 'S', [ 'S', /\\+/, 'M' ], 'M' );\nG.addRule( 'M', [ 'M', /\\*/, 'T' ], 'T' );\nG.addRule( 'T', /-?[0-9]+/ );\nJSON.stringify( G.parse( '15 + -2 * 9' ), null, 2 );\n\n\n\n\nThe result of the \nparse\n function is an array of valid parsings.  In this\ncase, it has only one element, because there is only one way the expression\ncould be parsed.  For more complex grammars with ambiguities, more than one\nresult may be returned.\n\n\nTo override options chosen with \nsetOption()\n, call the parse function with\nan optional second argument, an object whose keys and values override the\nexisting options.  Example:\n\n\nG.parse( 'text', { collapseBranches : true } );\n\n\n\n\nMiscellany\n\n\nThis module extends the global \nJSON\n object with a routine that can compare\ntwo JSON structures for structural equality.  Two atomic values are equal if\nthey are actually equal, two arrays are equal if they have the same lengths\nand their corresponding entries are equal, and two objects are equal if they\nhave the same keys and the corresponding values in each are equal.\n\n\n\nJSON.equals( [ 1, 2, { 3 : 4 } ], [ 1, 2, { 3 : 4 } ] );\n\n\n\n\n\nJSON.equals( { a : 5 }, { b : 5 } );\n\n\n\n\nMore Examples\n\n\nIn addition to the brief examples shown in this file, \nthe test suite in the\nsource code\nrepository\n\nis (naturally) a large set of examples of how the module works.\n\n\nWebWorker API\n\n\nThis section assumes that you have read and understood the API for the\nnon-WebWorker use of the module, as given in the previous sections.  It also\nassumes that you've read the \ngetting started section\n so\nthat you know how to import this module into a WebWorker.\n\n\nAssuming you've created a worker \nW\n as in that section, you can then\ninteract with it through five types of messages.\n\n\nCreate a new parser\n\n\nW.postMessage( [ \nnewParser\n, \nname here\n, \nstart token here\n ] );\n\n\n\n\nThis creates a new parser keyed by the given name, with no tokenizer,\noverwriting any old one with the same name.\n\n\nExample:\n\n\nW.postMessage( [ \nnewParser\n, \nmath expressions\n, \nterm\n ] );\n// No rules in this parser yet, just the parser itself.\n\n\n\n\nAdd a token type\n\n\nW.postMessage( [ \naddType\n, \nparser name\n, \nstring of token regexp\n,\n                 \noptional string of transform function code\n ] );\n\n\n\n\nThis adds a token type to the parser's tokenizer.  If the parser does not\nyet have a tokenizer, this first creates a new one.\n\n\nWhen adding a type to a tokenizer (as documented above) we normally provide\nthe regular expression for recognizing that token.  But regular expressions\ncannot be passed to WebWorkers, and so just their source as a string should\nbe passed instead.  See the example below.\n\n\nToken types can come with an optional transform function to be applied to\nany token recognized of that type.  That fourth parameter is optional and\nmay be omitted from the form shown above.  If present, it should be the\nstring representation of the function, so it will survive passage to the\nWebWorker.  See the example below.\n\n\nExample:\n\n\nwhiteSpaceRegExp = /\\s+/;\ndeleteWhiteSpace = function () { return null; }\nW.postMessage( [ \naddType\n, \nmath expressions\n, whiteSpaceRegExp.source,\n                 String( deleteWhiteSpace ) ] );\n\n\n\n\nAdd a grammar rule\n\n\nW.postMessage( [ \naddRule\n, \nparser name\n, \ncategory\n, sequences... ] );\n\n\n\n\nThis adds one or more rules to the parser.  Sending a message of this type\nleads directly to function calls of \naddRule()\n as documented above.  Thus\nyou can structure your sequences the same as in calls to \naddRule()\n,\nexcept for the following change.\n\n\nBecause regular expressions cannot be passed to WebWorkers, we modify the\nconvention for the sequences.\n\n\n\n\nWhere you would have represented a category by a string containing its\n   name \n\"cat\"\n you now represent it by a string containing its name, plus\n   the prefix to show that it is a category, \n\"c:cat\"\n.\n\n\nWhere you would have represented a terminal by a regular expression that\n   matches the terminal \n/fo+/\n, you now represent it by a string\n   containing the regular expression's source, plus the prefix to show that\n   it is a terminal, \n\"t:fo+\"\n.\n\n\n\n\nExample:\n\n\nW.postMessage( [ \naddRule\n, \nmath expressions\n, \ninteger\n, \nt:-?[0-9]+\n ] );\n\n\n\n\nParse text using a parser defined earlier\n\n\nW.onmessage = function ( event ) {\n    console.log( \nHeard back from the worker with this:\n, event.data );\n}\nW.sendMessage( [ \nparser name\n, \ntext to parse\n ] );\n\n\n\n\nThis instructs the worker to parse the given text with the named parser,\nwhich must have been defined earlier by messages of the \"newParser\",\n\"addType\", and \"addRule\" types.\n\n\nThe results are posted back to the main thread using \npostMessage()\n from\nwithin the worker thread, and thus the \nonmessage\n handler must be\nimplemented, as shown in the example code above.  The data in the \nevent\n\nwill be an array containing all valid parse trees for the given text in the\ngrammar of the named parser.\n\n\nThe trees are represented as nested JavaScript arrays whose first (\"head\")\nelements state the grammar categories contained in the grammar.  For\nexample, one parse tree might look like the following.  (Keep in mind that\neven if this were the only valid parse tree, it would still be wrapped in\nanother array to show that it was the one valid parsing of the given text.)\n\n\n[\n    \nsum\n,\n    [ \ninteger\n, \n5\n ],\n    [ \nproduct\n, [ \ninteger\n, \n6\n ], [ \nvariable\n, \nx\n ] ]\n]\n\n\n\n\nIf you choose not to have the module do tokenization for you, you can just\nnot send any messages of the \"addToken\" type, and then pass an array of\ntokens in place of the string of text to parse.\n\n\nExample:\n\n\nvar textToParse = \n12-6/x\n;\nW.onmessage = function ( event ) {\n    console.log( \nThe valid parse trees for\n, textToParse, \nare:\n );\n    if ( event.data.length == 0 )\n        console.log( \n(none)\n );\n    else\n        for ( var i = 0 ; i \n event.data.length ; i++ )\n            console.log( (i+1) + \n:\n, event.data[i] );\n}\nW.postMessage( [ \nparse\n, \nmath expression\n, textToParse ] );\n\n\n\n\nDelete an old parser\n\n\nW.postMessage( [ \ndeleteParser\n, \nname here\n ] );\n\n\n\n\nLets the worker reclaim memory by discarding parsers about which no further\nmessages will be passed.\n\n\nExample:\n\n\nW.postMessage( [ \ndeleteParser\n, \nmath expression\n ] );\n\n\n\n\n\n\n\n\nvar elements = document.getElementsByClassName( 'runnable-example' );\nfor ( var i = 0 ; i < elements.length ; i++ ) {\n    var source = elements[i].textContent;\n    elements[i].textContent = '';\n    var notebook = RunKit.createNotebook( {\n        element: elements[i],\n        source: source,\n        preamble: 'Tokenizer = require( \"earley-parser\" ).Tokenizer;\\nGrammar = require( \"earley-parser\" ).Grammar;'\n    } );\n}", 
            "title": "Reference"
        }, 
        {
            "location": "/api-reference/#api-reference", 
            "text": "", 
            "title": "API Reference"
        }, 
        {
            "location": "/api-reference/#getting-started", 
            "text": "", 
            "title": "Getting started"
        }, 
        {
            "location": "/api-reference/#in-the-browser", 
            "text": "Import the minified JavaScript, which you can  download from our repository\ndirectly \nor import from a CDN with the following one-liner.  script src='https://cdn.jsdelivr.net/npm/earley-parser@1/earley-parser.js' /script", 
            "title": "In the browser"
        }, 
        {
            "location": "/api-reference/#from-the-command-line", 
            "text": "Or install this package into your project the usual way:  npm install earley-parser  Then within any of your modules, import it as follows.  Tokenizer = require(  earley-parser  ).Tokenizer;\nGrammar = require(  earley-parser  ).Grammar;  After that, any of the example code snippets in this documentation should\nfunction as-is.", 
            "title": "From the command line"
        }, 
        {
            "location": "/api-reference/#in-a-webworker", 
            "text": "To place this script in a WebWorker, you will need to download  its source\nfile  and place it in your project's web space.  Your script can then create the worker as follows.  W = new Worker(  path/to/earley-parser.js  );  This exposes an asynchronous API documented  below .", 
            "title": "In a WebWorker"
        }, 
        {
            "location": "/api-reference/#tokenizing", 
            "text": "Traditionally, parsing text is split first into a \"tokenization\" phase, in\nwhich chunks of text are recognized as atomic units, and thus the string\nbecomes an array of its substrings, each of size 1 or greater, followed by\nthe parsing phase, which operates on that flat array, arranging it into a\ntree structure.  For example, in standard arithmetic, the text \"1+23*5\" might first be\ntokenized into the array  [ \"1\", \"+\", \"23\", \"*\", \"5\" ] , and then parsed\ninto the (prefix-notation) tree structure [ \"+\", \"1\", [ \"*\", \"23\", \"5\" ] ] .  A tokenizer is therefore an ordered list of regular expressions for\ndetecting tokens, popping them off of an input string, and then possibly\nmanipulating them before adding them to a growing array of tokens found in\nthe string.  This module provides a class for creating tokenizers.", 
            "title": "Tokenizing"
        }, 
        {
            "location": "/api-reference/#constructor", 
            "text": "The constructor takes no parameters, so it is very simple to use.  T = new Tokenizer();", 
            "title": "Constructor"
        }, 
        {
            "location": "/api-reference/#adding-types-to-a-tokenizer", 
            "text": "There is precisely one function for adding types to the tokenizer.  Note,\nhowever, that types are checked in the order in which they were added to the\ntokenizer, so when you call this function repeatedly to add various types of\ntokens, you should take care to order the calls correctly.  For instance, perhaps you are writing a tokenizer for simple algebraic\nexpressions, in which a sequence of letters will be seen as the\nmultiplication of variables (i.e., \"abc\" means \"a times b times c\") except\nfor a few special sequences of letters such as \"sin\" and \"cos\" and \"pi\" and\nperhaps a few others.  It is important to add the tokens for \"sin\" and \"cos\" and so on first, and\nthen the generic single-letter token thereafter, so that the special cases\nhave a chance to be applied before the general case.  Otherwise the general\ncase would catch all sequences of letters, and the special cases would\nnever have a chance to be applied.  The function signature looks like so:  T.addType( regexp, formatter );  The two parameters are documented thoroughly in the source code documentation , so I do not repeat the information here.", 
            "title": "Adding types to a tokenizer"
        }, 
        {
            "location": "/api-reference/#tokenizing_1", 
            "text": "To tokenize text, simply call  T.tokenize( input )  on the text.  Example:  \nT = new Tokenizer();\nT.addType( /sin/ );\nT.addType( /cos/ );\nT.addType( /pi/ );\nT.addType( /[a-z]/, function ( name ) { return \"Variable:\" + name; } );\nT.addType( /\\s/, function () { return null; } );\nconsole.log( T.tokenize( 'sin x' ) );\nconsole.log( T.tokenize( 'cospiy' ) );", 
            "title": "Tokenizing"
        }, 
        {
            "location": "/api-reference/#parsing", 
            "text": "A grammar is a set of rules defining the language to parse.  For more\ninformation on context-free grammars, see  the Wikipedia article on the\nEarley parser .  For the sake of having a concrete running example in this section, let's\nassume we want to want to create the extremely simple grammar used as  an\nexample in that same\narticle .  We will make\none modification: we will accept any integer, rather than just the four\ndigits in that example.  Our grammar can thus be summarized as the following\nrules.   P ::= S  S ::= S+M | M  M ::= M*T | T  T ::= any integer   We can represent the same grammar without the | symbol by separating single\nlines into two separate lines.   P ::= S  S ::= S+M  S ::= M  M ::= M*T  M ::= T  T ::= any integer   Either way of representing a grammar is acceptable, and supported by this\nmodule.  See  the section on specifying grammar\nrules , below.", 
            "title": "Parsing"
        }, 
        {
            "location": "/api-reference/#constructor_1", 
            "text": "There is just one constructor for grammars, and it takes as its sole\nargument the name of the start nonterminal.  This need not be a single\ncapital letter, as it is in the example above; nonterminals can have any\nword as their name.  G = new Grammar( 'P' );", 
            "title": "Constructor"
        }, 
        {
            "location": "/api-reference/#setting-default-options", 
            "text": "After constructing a new grammar, you can choose to set some default options\nthat will govern its behavior.  Any of these can be overridden in any call\nto the parse function, later, but you can set defaults here if you plan to\nneed them often.  You call  G.setOption( name, value )  to set the default\nvalue for any option.  The options are documented thoroughly in  the source code\ndocumentation ,\nso I do not repeat that information here.  Examples of the output produced\nby the various options appears in  the parsing\nsection , below.", 
            "title": "Setting default options"
        }, 
        {
            "location": "/api-reference/#adding-grammar-rules", 
            "text": "A grammar rule requires a left-hand side, which must be a single\nnonterminal, represented by a string.  Its right-hand side is typically an\narray.  For instance, the grammar rule M ::= M T has M as its left-hand side\nand the three-element array M,  , and T as its right hand side.  The elements on the right hand side come in two types.  There are other\nnonterminals (such as M and T) and there are terminals (the symbol  , in\nthis example).  Nonterminals are represented as strings, and terminals as\nregular expressions.  Thus to create the grammar rule M ::= M T, we would\nuse  'M'  as the left-hand side and  [ 'M', /\\*/, 'T' ]  as the right-hand\nside.  Our complete example grammar can then be created as follows.  G = new Grammar( 'P' );\nG.addRule( 'P', [ 'S' ] );\nG.addRule( 'S', [ 'S', /\\+/, 'M' ] );\nG.addRule( 'S', [ 'M' ] );\nG.addRule( 'M', [ 'M', /\\*/, 'T' ] );\nG.addRule( 'M', [ 'T' ] );\nG.addRule( 'T', [ /-?[0-9]+/ ] );  There are a few things to improve upon here.   We may combine rules that have the same left-hand side, just be listing\n    the right-hand sides one after the other.  We may express a one-element array containing a terminal by the regular\n    expression alone, omitting the enclosing array.  We may express a right-hand side consisting exclusively of one or more\n    nonterminals by writing it as a single string, with the names of the\n    nonterminals separated by spaces.   This reduces our grammar somewhat.  G = new Grammar( 'P' );\nG.addRule( 'P', 'S' );\nG.addRule( 'S', [ 'S', /\\+/, 'M' ], 'M' );\nG.addRule( 'M', [ 'M', /\\*/, 'T' ], 'T' );\nG.addRule( 'T', /-?[0-9]+/ );  The P in this grammar is redundant; we could have left it out and declared\nour grammar to start with S, but I leave it in to be consistent with the\nWikipedia article from which it was taken.", 
            "title": "Adding grammar rules"
        }, 
        {
            "location": "/api-reference/#running-the-parser", 
            "text": "We can then parse text by passing it to the grammar's  parse  member\nfunction.  \nT = new Tokenizer();\nT.addType( /-?[0-9]+/ );\nT.addType( /[+*]/ );\nT.addType( /\\s/, function () { return null; } );\nG = new Grammar( 'P' );\nG.setOption( 'tokenizer', T );\n// Comment out either of these lines and re-run to see their effects:\nG.setOption( 'addCategories', false );\nG.setOption( 'collapseBranches', true );\nG.addRule( 'P', 'S' );\nG.addRule( 'S', [ 'S', /\\+/, 'M' ], 'M' );\nG.addRule( 'M', [ 'M', /\\*/, 'T' ], 'T' );\nG.addRule( 'T', /-?[0-9]+/ );\nJSON.stringify( G.parse( '15 + -2 * 9' ), null, 2 );  The result of the  parse  function is an array of valid parsings.  In this\ncase, it has only one element, because there is only one way the expression\ncould be parsed.  For more complex grammars with ambiguities, more than one\nresult may be returned.  To override options chosen with  setOption() , call the parse function with\nan optional second argument, an object whose keys and values override the\nexisting options.  Example:  G.parse( 'text', { collapseBranches : true } );", 
            "title": "Running the parser"
        }, 
        {
            "location": "/api-reference/#miscellany", 
            "text": "This module extends the global  JSON  object with a routine that can compare\ntwo JSON structures for structural equality.  Two atomic values are equal if\nthey are actually equal, two arrays are equal if they have the same lengths\nand their corresponding entries are equal, and two objects are equal if they\nhave the same keys and the corresponding values in each are equal.  \nJSON.equals( [ 1, 2, { 3 : 4 } ], [ 1, 2, { 3 : 4 } ] );  \nJSON.equals( { a : 5 }, { b : 5 } );", 
            "title": "Miscellany"
        }, 
        {
            "location": "/api-reference/#more-examples", 
            "text": "In addition to the brief examples shown in this file,  the test suite in the\nsource code\nrepository \nis (naturally) a large set of examples of how the module works.", 
            "title": "More Examples"
        }, 
        {
            "location": "/api-reference/#webworker-api", 
            "text": "This section assumes that you have read and understood the API for the\nnon-WebWorker use of the module, as given in the previous sections.  It also\nassumes that you've read the  getting started section  so\nthat you know how to import this module into a WebWorker.  Assuming you've created a worker  W  as in that section, you can then\ninteract with it through five types of messages.", 
            "title": "WebWorker API"
        }, 
        {
            "location": "/api-reference/#create-a-new-parser", 
            "text": "W.postMessage( [  newParser ,  name here ,  start token here  ] );  This creates a new parser keyed by the given name, with no tokenizer,\noverwriting any old one with the same name.  Example:  W.postMessage( [  newParser ,  math expressions ,  term  ] );\n// No rules in this parser yet, just the parser itself.", 
            "title": "Create a new parser"
        }, 
        {
            "location": "/api-reference/#add-a-token-type", 
            "text": "W.postMessage( [  addType ,  parser name ,  string of token regexp ,\n                  optional string of transform function code  ] );  This adds a token type to the parser's tokenizer.  If the parser does not\nyet have a tokenizer, this first creates a new one.  When adding a type to a tokenizer (as documented above) we normally provide\nthe regular expression for recognizing that token.  But regular expressions\ncannot be passed to WebWorkers, and so just their source as a string should\nbe passed instead.  See the example below.  Token types can come with an optional transform function to be applied to\nany token recognized of that type.  That fourth parameter is optional and\nmay be omitted from the form shown above.  If present, it should be the\nstring representation of the function, so it will survive passage to the\nWebWorker.  See the example below.  Example:  whiteSpaceRegExp = /\\s+/;\ndeleteWhiteSpace = function () { return null; }\nW.postMessage( [  addType ,  math expressions , whiteSpaceRegExp.source,\n                 String( deleteWhiteSpace ) ] );", 
            "title": "Add a token type"
        }, 
        {
            "location": "/api-reference/#add-a-grammar-rule", 
            "text": "W.postMessage( [  addRule ,  parser name ,  category , sequences... ] );  This adds one or more rules to the parser.  Sending a message of this type\nleads directly to function calls of  addRule()  as documented above.  Thus\nyou can structure your sequences the same as in calls to  addRule() ,\nexcept for the following change.  Because regular expressions cannot be passed to WebWorkers, we modify the\nconvention for the sequences.   Where you would have represented a category by a string containing its\n   name  \"cat\"  you now represent it by a string containing its name, plus\n   the prefix to show that it is a category,  \"c:cat\" .  Where you would have represented a terminal by a regular expression that\n   matches the terminal  /fo+/ , you now represent it by a string\n   containing the regular expression's source, plus the prefix to show that\n   it is a terminal,  \"t:fo+\" .   Example:  W.postMessage( [  addRule ,  math expressions ,  integer ,  t:-?[0-9]+  ] );", 
            "title": "Add a grammar rule"
        }, 
        {
            "location": "/api-reference/#parse-text-using-a-parser-defined-earlier", 
            "text": "W.onmessage = function ( event ) {\n    console.log(  Heard back from the worker with this: , event.data );\n}\nW.sendMessage( [  parser name ,  text to parse  ] );  This instructs the worker to parse the given text with the named parser,\nwhich must have been defined earlier by messages of the \"newParser\",\n\"addType\", and \"addRule\" types.  The results are posted back to the main thread using  postMessage()  from\nwithin the worker thread, and thus the  onmessage  handler must be\nimplemented, as shown in the example code above.  The data in the  event \nwill be an array containing all valid parse trees for the given text in the\ngrammar of the named parser.  The trees are represented as nested JavaScript arrays whose first (\"head\")\nelements state the grammar categories contained in the grammar.  For\nexample, one parse tree might look like the following.  (Keep in mind that\neven if this were the only valid parse tree, it would still be wrapped in\nanother array to show that it was the one valid parsing of the given text.)  [\n     sum ,\n    [  integer ,  5  ],\n    [  product , [  integer ,  6  ], [  variable ,  x  ] ]\n]  If you choose not to have the module do tokenization for you, you can just\nnot send any messages of the \"addToken\" type, and then pass an array of\ntokens in place of the string of text to parse.  Example:  var textToParse =  12-6/x ;\nW.onmessage = function ( event ) {\n    console.log(  The valid parse trees for , textToParse,  are:  );\n    if ( event.data.length == 0 )\n        console.log(  (none)  );\n    else\n        for ( var i = 0 ; i   event.data.length ; i++ )\n            console.log( (i+1) +  : , event.data[i] );\n}\nW.postMessage( [  parse ,  math expression , textToParse ] );", 
            "title": "Parse text using a parser defined earlier"
        }, 
        {
            "location": "/api-reference/#delete-an-old-parser", 
            "text": "W.postMessage( [  deleteParser ,  name here  ] );  Lets the worker reclaim memory by discarding parsers about which no further\nmessages will be passed.  Example:  W.postMessage( [  deleteParser ,  math expression  ] );   \nvar elements = document.getElementsByClassName( 'runnable-example' );\nfor ( var i = 0 ; i < elements.length ; i++ ) {\n    var source = elements[i].textContent;\n    elements[i].textContent = '';\n    var notebook = RunKit.createNotebook( {\n        element: elements[i],\n        source: source,\n        preamble: 'Tokenizer = require( \"earley-parser\" ).Tokenizer;\\nGrammar = require( \"earley-parser\" ).Grammar;'\n    } );\n}", 
            "title": "Delete an old parser"
        }
    ]
}